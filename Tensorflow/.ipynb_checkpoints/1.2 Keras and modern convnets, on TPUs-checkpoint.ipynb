{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Overview\n",
    "In this lab, you will learn how to build, train and tune your own convolutional neural networks from scratch with Keras and Tensorflow 2. This can now be done in minutes using the power of TPUs. You will also explore multiple approaches from very simple transfer learning to modern convolutional architectures such as Squeezenet. This lab includes theoretical explanations about neural networks and is a good starting point for developers learning about deep learning.\n",
    "Reading deep learning papers can be hard and confusing. Let us have a hands-on look at modern convolutional neural network architectures. \n",
    "\n",
    "What you'll learn\n",
    "To use Keras and Tensor Processing Units (TPUs) to build your custom models faster.  \n",
    "To use the tf.data.Dataset API and the TFRecord format to load training data efficiently.  \n",
    "To cheat ðŸ˜ˆ, using transfer learning instead of building your own models.  \n",
    "To use Keras sequential and functional model styles.  \n",
    "To build your own Keras classifier with a softmax layer and cross-entropy loss.  \n",
    "To fine-tune your model with a good choice of convolutional layers.  \n",
    "To explore modern convnet architecture ideas like modules, global average pooling, etc.  \n",
    "To build a simple modern convnet using the Squeezenet architecture.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  2.Google Colaboratory quick start\n",
    "\n",
    "This lab uses Google Collaboratory and requires no setup on your part. You can run it from a Chromebook. Please open the file below, and execute the cells to familiarize yourself with Colab notebooks.  \n",
    "[Welcome to Colab.ipynb](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/colab_intro.ipynb#scrollTo=vq9yuTXI9SZK)  \n",
    "## Select a TPU backend \n",
    "![](./img/notebook_settings.png)  \n",
    "In the Colab menu, select **Runtime > Change runtime type** and then select TPU. In this code lab you will use a powerful TPU (Tensor Processing Unit) backed for hardware-accelerated training. Connection to the runtime will happen automatically on first execution, or you can use the \"Connect\" button in the upper-right corner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook execution\n",
    "![](./img/notebook_execution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute cells one at a time by clicking on a cell and using Shift-ENTER. You can also run the entire notebook with **Runtime > Run all**\n",
    "\n",
    "## Table of contents\n",
    "![](./img/table_of_contents.png)\n",
    "\n",
    "\n",
    "All notebooks have a table of contents. You can open it using the black arrow on the left.\n",
    "\n",
    "## Hidden cells\n",
    "![](./img/hidden_cells.png) \n",
    "Some cells will only show their title. This is a Colab-specific notebook feature. You can double click on them to see the code inside but it is usually not very interesting. Typically support or visualization functions. You still need to run these cells for the functions inside to be defined.\n",
    "\n",
    "## Authentication\n",
    "![](./img/Authentication.png)  \n",
    "It is possible for Colab to access your private Google Cloud Storage buckets provided you authenticate with an authorized account. The code snippet above will trigger an authentication process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. [INFO] What are Tensor Processing Units (TPUs) ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/TPUs.png)  \n",
    ">TPUs are hardware accelerators specialized in deep learning tasks. In this code lab, you will see how to use them with Keras and Tensorflow 2. Cloud TPUs are available in a base configuration with 8 cores and also in larger configurations called \"TPU pods\" of up to 2048 cores. The extra hardware can be used to accelerate training by increasing the training batch size.\n",
    "\n",
    "The code for training a model on TPU in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect the TPU\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "# use TPUStrategy scope to define model\n",
    "with strategy.scope():\n",
    "  model = tf.keras.Sequential( ... )\n",
    "  model.compile( ... )\n",
    "\n",
    "# train model normally on a tf.data.Dataset\n",
    "model.fit(training_dataset, epochs=EPOCHS, steps_per_epoch=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use TPUs today to build and optimize a flower classifier at interactive speeds (minutes per training run).\n",
    "\n",
    "## Why TPUs ?\n",
    "Modern GPUs are organized around programmable \"cores\", a very flexible architecture that allows them to handle a variety of tasks such as 3D rendering, deep learning, physical simulations, etc.. TPUs on the other hand pair a classic vector processor with a dedicated matrix multiply unit and excel at any task where large matrix multiplications dominate, such as neural networks.\n",
    "\n",
    "![](./img/matrix_multiplication.png)   \n",
    "Illustration: a dense neural network layer as a matrix multiplication, with a batch of eight images processed through the neural network at once. Please run through one line x column multiplication to verify that it is indeed doing a weighted sum of all the pixels values of an image. Convolutional layers can be represented as matrix multiplications too although it's a bit more complicated ([explanation here, in section 1](http://www.telesens.co/2018/04/09/initializing-weights-for-the-convolutional-and-fully-connected-layers/).)\n",
    "\n",
    "## The hardware\n",
    "### MXU and VPU\n",
    "A TPU v2 core is made of a Matrix Multiply Unit (MXU) which runs matrix multiplications and a Vector Processing Unit (VPU) for all other tasks such as activations, softmax, etc. The VPU handles float32 and int32 computations. The MXU on the other hand operates in a mixed precision 16-32 bit floating point format.\n",
    "![](./img/TPU_hardware.png) \n",
    "\n",
    "### Mixed precision floating point and bfloat16\n",
    "The MXU computes matrix multiplications using bfloat16 inputs and float32 outputs. Intermediate accumulations are performed in float32 precision.\n",
    "![](./img/bfloat16.png)   \n",
    "Neural network training is typically resistant to the noise introduced by a reduced floating point precision. There are cases where noise even helps the optimizer converge. 16-bit floating point precision has traditionally been used to accelerate computations but float16 and float32 formats have very different ranges. Reducing the precision from float32 to float16 usually results in over and underflows. Solutions exist but additional work is typically required to make float16 work.\n",
    "That is why Google introduced the bfloat16 format in TPUs. bfloat16 is a truncated float32 with exactly the same exponent bits and range as float32. This, added to the fact that TPUs compute matrix multiplications in mixed precision with bfloat16 inputs but float32 outputs, means that, typically, no code changes are necessary to benefit from the performance gains of reduced precision.\n",
    ">The use of bfloat16/float32 mixed precision is the default on TPUs. No code changes are necessary in your Tensorflow code to enable it. You still work with float32 throughout your code. When executing a matrix multiplication, the TPU will automatically truncate its inputs to bfloat16. The resulting matrix is float32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systolic array\n",
    "The MXU implements matrix multiplications in hardware using a so-called \"systolic array\" architecture in which data elements flow through an array of hardware computation units. (In medicine, \"systolic\" refers to heart contractions and blood flow, here to the flow of data.)  \n",
    "The basic element of a matrix multiplication is a dot product between a line from one matrix and a column from the other matrix (see illustration at the top of this section). For a matrix multiplication Y=X*W, one element of the result would be:  \n",
    "$$Y[2,0] = X[2,0]*W[0,0] + X[2,1]*W[1,0] + X[2,2]*W[2,0] + ... + X[2,n]*W[n,0]$$  \n",
    "On a GPU, one would program this dot product into a GPU \"core\" and then execute it on as many \"cores\" as are available in parallel to try and compute every value of the resulting matrix at once. If the resulting matrix is 128x128 large, that would require 128x128=16K \"cores\" to be available which is typically not possible. The largest GPUs have around 4000 cores. A TPU on the other hand uses the bare minimum of hardware for the compute units in the MXU: just bfloat16 x bfloat16 => float32 multiply-accumulators, nothing else. These are so small that a TPU can implement 16K of them in a 128x128 MXU and process this matrix multiplication in one go.\n",
    "![](./img/systolic_array.gif)\n",
    "*Illustration: the MXU systolic array. The compute elements are multiply-accumulators. The values of one matrix are loaded into the array (red dots). Values of the other matrix flow through the array (grey dots). Vertical lines propagate the values up. Horizontal lines propagate partial sums. It is left as an exercise to the user to verify that as the data flows through the array, you get the result of the matrix multiplication coming out of the right side.*  \n",
    "\n",
    "In addition to that, while the dot products are being computed in an MXU, intermediate sums simply flow between adjacent compute units. They do not need to be stored and retrieved to/from memory or even a register file. The end result is that the TPU systolic array architecture has a significant density and power advantage, as well as a non-negligible speed advantage over a GPU, when computing matrix multiplications.\n",
    "\n",
    "### Cloud TPU\n",
    "When you request one \"[Cloud TPU v2](https://cloud.google.com/tpu/)\" on Google Cloud Platform, you get a virtual machine (VM) which has a PCI-attached TPU board. The TPU board has four dual-core TPU chips. Each TPU core features a VPU (Vector Processing Unit) and a 128x128 MXU (MatriX multiply Unit). This \"Cloud TPU\" is then usually connected through the network to the VM that requested it. So the full picture looks like this:\n",
    "![](./img/host_vm.png) \n",
    "*Illustration: your VM with a network-attached \"Cloud TPU\" accelerator. \"The Cloud TPU\" itself is made of a VM with a PCI-attached TPU board with four dual-core TPU chips on it.*\n",
    "\n",
    "### TPU pods\n",
    "In Google's data centers, TPUs are connected to a high-performance computing (HPC) interconnect which can make them appear as one very large accelerator. Google calls them pods and they can encompass up to 512 TPU v2 cores or 2048 TPU v3 cores.. \n",
    "![](./img/TPU_v3_pod.png) \n",
    "*Illustration: a TPU v3 pod. TPU boards and racks connected through HPC interconnect.*  \n",
    "During training, gradients are exchanged between TPU cores using the all-reduce algorithm ([good explanation of all-reduce here](https://www.logicalclocks.com/goodbye-horovod-hello-tensorflow-collectiveallreduce/ï¼‰). The model being trained can take advantage of the hardware by training on large batch sizes.\n",
    "![](./img/all-reduce_animation.gif)\n",
    "*Illustration: synchronization of gradients during training using the all-reduce algorithm on Google TPU's 2-D toroidal mesh HPC network.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The software\n",
    "### Large batch size training\n",
    "The ideal batch size for TPUs is 128 data items per TPU core but the hardware can already show good utilization from 8 data items per TPU core. Remember that one Cloud TPU has 8 cores.\n",
    "In this code lab, we will be using the Keras API. In Keras, the batch you specify is the global batch size for the entire TPU. Your batches will automatically be split in 8 and ran on the 8 cores of the TPU.\n",
    "![](./img/TPU_batch_size2.png)\n",
    "For additional performance tips see the [TPU Performance Guide](https://cloud.google.com/tpu/docs/performance-guide). For very large batch sizes, special care might be needed in some models, see [LARSOptimizer](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/opt/python/training/lars_optimizer.py) for more details.  \n",
    "\n",
    "### Under the hood: XLA\n",
    "Tensorflow programs define computation graphs. The TPU does not directly run Python code, it runs the computation graph defined by your Tensorflow program. Under the hood, a compiler called XLA (accelerated Linear Algebra compiler) transforms the Tensorflow graph of computation nodes into TPU machine code. This compiler also performs many advanced optimizations on your code and your memory layout. The compilation happens automatically as work is sent to the TPU. You do not have to include XLA in your build chain explicitly.\n",
    "![](./img/XLA_TPU.png) \n",
    "*Illustration: to run on TPU, the computation graph defined by your Tensorflow program is first translated to an XLA (accelerated Linear Algebra compiler) representation, then compiled by XLA into TPU machine code.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TPUs in Keras\n",
    "TPUs are supported through the Keras API as of Tensorflow 2.1. Keras support works on TPUs and TPU pods. Here is an example that works on TPU, GPU(s) and CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPU detection  \n",
    "try:\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "except ValueError:\n",
    "  tpu = None\n",
    "\n",
    "# TPUStrategy for distributed training\n",
    "if tpu:\n",
    "  tf.config.experimental_connect_to_cluster(tpu)\n",
    "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else: # default strategy that works on CPU and single GPU\n",
    "  strategy = tf.distribute.get_strategy()\n",
    "\n",
    "# use TPUStrategy scope to define model\n",
    "with strategy.scope():\n",
    "  model = tf.keras.Sequential( ... )\n",
    "  model. compile( ... )\n",
    "\n",
    "# train model normally on a tf.data.Dataset\n",
    "model.fit(training_dataset, epochs=EPOCHS, steps_per_epoch=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code snippet:  \n",
    "*TPUClusterResolver() finds the TPU on the network. It works without parameters on most Google Cloud systems (AI Platform jobs, Colaboratory, Kubeflow, Deep Learning VMs created through the â€˜ctpu up' utility). These systems know where their TPU is thanks to a TPU_NAME environment variable. If you create a TPU by hand, eithert set the TPU_NAME env. var. on the VM you are using it from, or call TPUClusterResolver with explicit parameters: TPUClusterResolver(tp_uname, zone, project)  \n",
    "*TPUStrategy is the part that implements the distribution and the \"all-reduce\" gradient synchronization algorithm.  \n",
    "*The strategy is applied through a scope. The model must be defined within the strategy scope().  \n",
    "*The tpu_model.fit function expects a tf.data.Dataset object for input for TPU training.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common TPU porting tasks\n",
    "*While there are many ways to load data in a Tensorflow model, for TPUs, the use of the tf.data.Dataset API is required.\n",
    "*TPUs are very fast and ingesting data often becomes the bottleneck when running on them. There are tools you can use to detect data bottlenecks and other performance tips in the [TPU Performance Guide](https://cloud.google.com/tpu/docs/performance-guide).\n",
    "*int8 or int16 numbers are treated as int32. The TPU does not have integer hardware operating on less than 32 bits.\n",
    "*Some Tensorflow operations are not supported. The [list is here](https://cloud.google.com/tpu/docs/tensorflow-ops). The good news is that this limitation only applies to training code i.e. the forward and backward pass through your model. You can still use all Tensorflow operations in your data input pipeline as it will be executed on CPU.\n",
    "*[tf.py_func](https://www.tensorflow.org/api_docs/python/tf/py_func) is not supported on TPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loading Data\n",
    "![](./img/flower.png)\n",
    "We will be working with a dataset of flower pictures. The goal is to learn to categorize them into 5 flower types. Data loading is performed using the tf.data.Dataset API. First, let us get to know the API.\n",
    "## Hands-on\n",
    "Please open the following notebook, execute the cells (Shift-ENTER) and follow the instructions wherever you see a \"WORK REQUIRED\" label.  \n",
    "[Fun with tf.data.Dataset (playground).ipynb](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/02_Dataset_playground.ipynb) \n",
    "\n",
    "## Additional information\n",
    "### About the \"flowers\" dataset\n",
    "The dataset is organised in 5 folders. Each folder contains flowers of one kind. The folders are named sunflowers, daisy, dandelion, tulips and roses. The data is hosted in a public bucket on Google Cloud Storage. Excerpt:  \n",
    ">gs://flowers-public/sunflowers/5139971615_434ff8ed8b_n.jpg\n",
    "gs://flowers-public/daisy/8094774544_35465c1c64.jpg\n",
    "gs://flowers-public/sunflowers/9309473873_9d62b9082e.jpg\n",
    "gs://flowers-public/dandelion/19551343954_83bb52f310_m.jpg\n",
    "gs://flowers-public/dandelion/14199664556_188b37e51e.jpg\n",
    "gs://flowers-public/tulips/4290566894_c7f061583d_m.jpg\n",
    "gs://flowers-public/roses/3065719996_c16ecd5551.jpg\n",
    "gs://flowers-public/dandelion/8168031302_6e36f39d87.jpg\n",
    "gs://flowers-public/sunflowers/9564240106_0577e919da_n.jpg\n",
    "gs://flowers-public/daisy/14167543177_cd36b54ac6_n.jpg\n",
    "\n",
    "Why tf.data.Dataset?\n",
    "Keras and Tensorflow accept Datasets in all of their training and evaluation functions. Once you load data in a Dataset, the API offers all the common functionalities that are useful for neural network training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ... # load something (see below)\n",
    "dataset = dataset.shuffle(1000) # shuffle the dataset with a buffer of 1000\n",
    "dataset = dataset.cache() # cache the dataset in RAM or on disk\n",
    "dataset = dataset.repeat() # repeat the dataset indefinitely\n",
    "dataset = dataset.batch(128) # batch data elements together in batches of 128\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "dataset = dataset.prefetch(AUTO) # prefetch next batch(es) while training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find performance tips and Dataset best practices in this article. The reference documentation is here.\n",
    "### tf.data.Dataset basics\n",
    "Data usually comes in multiple files, here images. You can create a dataset of filenames by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_dataset = tf.data.Dataset.list_files('gs://flowers-public/*/*.jpg')\n",
    "# The parameter is a \"glob\" pattern that supports the * and ? wildcards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You then \"map\" a function to each filename which will typically load and decode the file into actual data in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_jpeg(filename):\n",
    "  bits = tf.io.read_file(filename)\n",
    "  image = tf.image.decode_jpeg(bits)\n",
    "  return image\n",
    "\n",
    "image_dataset = filenames_dataset.map(decode_jpeg)\n",
    "# this is now a dataset of decoded images (uint8 RGB format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To iterate on a Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in my_dataset:\n",
    "  print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets of tuples\n",
    "In supervised learning, a training dataset is typically made of pairs of training data and correct answers. To allow this, the decoding function can return tuples. You will then have a dataset of tuples and tuples will be returned when you iterate on it. The values returned are Tensorflow tensors ready to be consumed by your model. You can call .numpy() on them to see raw values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_jpeg_and_label(filename):\n",
    "  bits = tf.read_file(filename)\n",
    "  image = tf.image.decode_jpeg(bits)\n",
    "  label = ... # extract flower name from folder name\n",
    "  return image, label\n",
    "\n",
    "image_dataset = filenames_dataset.map(decode_jpeg)\n",
    "# this is now a dataset of (image, label) pairs \n",
    "\n",
    "for image, label in dataset:\n",
    "  print(image.numpy().shape, label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:loading images one by one is slow !\n",
    "As you iterate on this dataset, you will see that you can load something like 1-2 images per second. That is too slow! The hardware accelerators we will be using for training can sustain many times this rate. Head to the next section to see how we will achieve this.\n",
    "## Solution \n",
    "Here is the solution notebook. You can use it if you are stuck\n",
    "[Fun with tf.data.Dataset (solution).ipynb](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/02_Dataset_solution.ipynb)\n",
    "What we've covered  \n",
    "ðŸ¤” tf.data.Dataset.list_files  \n",
    "ðŸ¤” tf.data.Dataset.map  \n",
    "ðŸ¤” Datasets of tuples  \n",
    "ðŸ˜€ iterating through Datasets  \n",
    "Please take a moment to go through this checklist in your head.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Loading data fast\n",
    "The Tensor Processing Unit (TPU) hardware accelerators we will be using in this lab are very fast. The challenge is often to feed them data fast enough to keep them busy. Google Cloud Storage (GCS) is capable of sustaining very high throughput but as with all cloud storage systems, initiating a connection costs some network back and forth. Therefore, having our data stored as thousands of individual files is not ideal. We are going to batch them in a smaller number of files and use the power of tf.data.Dataset to read from multiple files in parallel.\n",
    "## Read-through\n",
    "The code that loads image files, resizes them to a common size and then stores them across 16 TFRecord files is in the following notebook. Please quickly read through it. Executing it is not necessary since properly TFRecord-formatted data will be provided for the rest of the codelab.  \n",
    "[Flower pictures to TFRecords.ipynb](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/03_Flower_pictures_to_TFRecords.ipynb)\n",
    "\n",
    "## Ideal data layout for optimal GCS throughput  \n",
    ">The rule of thumb is to split your data across several (10s to 100s) larg-ish files (10s to 100s of MB). If you have too many files, thousands of files for example, the time to access each file might start getting in the way. If you have too few files, like one or two, then you are not getting the benefits of streaming from multiple files in parallel.\n",
    "\n",
    "## The TFRecord file format\n",
    "Tensorflow's preferred file format for storing data is the [protobuf](https://developers.google.com/protocol-buffers/)-based TFRecord format. Other serialization formats would work too but you can load a dataset from TFRecord files directly by writing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = tf.io.gfile.glob(FILENAME_PATTERN)\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(...) # do the TFRecord decoding here - see below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For optimal performance, it is recommended to use the following more complex code to read from multiple TFRecord files at once. This code will read from N files in parallel and disregard data order in favor of reading speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "ignore_order = tf.data.Options()\n",
    "ignore_order.experimental_deterministic = False\n",
    "\n",
    "filenames = tf.io.gfile.glob(FILENAME_PATTERN)\n",
    "dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
    "dataset = dataset.with_options(ignore_order)\n",
    "dataset = dataset.map(...) # do the TFRecord decoding here - see below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecord cheat sheet\n",
    "Three types of data can be stored in TFRecords: byte strings (list of bytes), 64 bit integers and 32 bit floats. They are always stored as lists, a single data element will be a list of size 1. You can use the following helper functions to store data into TFRecords.\n",
    "### writing byte strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warning, the input is a list of byte strings, which are themselves lists of bytes\n",
    "def _bytestring_feature(list_of_bytestrings):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### writing integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int_feature(list_of_ints): # int64\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### writing floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _float_feature(list_of_floats): # float32\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### writing a TFRecord,\n",
    "using the helpers above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data in my_img_bytes, my_class, my_height, my_width, my_floats\n",
    "with tf.python_io.TFRecordWriter(filename) as out_file:\n",
    "  feature = {\n",
    "    \"image\": _bytestring_feature([my_img_bytes]), # one image in the list\n",
    "    \"class\": _int_feature([my_class]),            # one class in the list\n",
    "    \"size\": _int_feature([my_height, my_width]),  # fixed length (2) list of ints\n",
    "    \"float_data\": _float_feature(my_floats)       # variable length  list of floats\n",
    "  }\n",
    "  tf_record = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "  out_file.write(tf_record.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read data from TFRecords, you must first declare the layout of the records you have stored. In the declaration, you can access any named field as a fixed length list or a variable length list:\n",
    "### reading from TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(data):\n",
    "  features = {\n",
    "    # tf.string = byte string (not text string)\n",
    "    \"image\": tf.io.FixedLenFeature([], tf.string), # shape [] means scalar, here, a single byte string\n",
    "    \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar, i.e. a single item\n",
    "    \"size\": tf.io.FixedLenFeature([2], tf.int64),  # two integers\n",
    "    \"float_data\": tf.io.VarLenFeature(tf.float32)  # a variable number of floats\n",
    "  }\n",
    "\n",
    "  # decode the TFRecord\n",
    "  tf_record = tf.parse_single_example(data, features)\n",
    "\n",
    "  # FixedLenFeature fields are now ready to use\n",
    "  sz = tf_record['size']\n",
    "\n",
    "  # Typical code for decoding compressed images\n",
    "  image = tf.image.decode_jpeg(tf_record['image'], channels=3)\n",
    "\n",
    "  # VarLenFeature fields require additional sparse.to_dense decoding\n",
    "  float_data = tf.sparse.to_dense(tf_record['float_data'])\n",
    "\n",
    "  return image, sz, float_data\n",
    "# decoding a tf.data.TFRecordDataset\n",
    "dataset = dataset.map(read_tfrecord)\n",
    "# now a dataset of triplets (image, sz, float_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful code snippets:\n",
    "### reading single data elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.io.FixedLenFeature([], tf.string)   # for one byte string\n",
    "tf.io.FixedLenFeature([], tf.int64)    # for one int\n",
    "tf.io.FixedLenFeature([], tf.float32)  # for one float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading fixed size lists of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.io.FixedLenFeature([N], tf.string)   # list of N byte strings\n",
    "tf.io.FixedLenFeature([N], tf.int64)    # list of N ints\n",
    "tf.io.FixedLenFeature([N], tf.float32)  # list of N floats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading a variable number of data items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.io.VarLenFeature(tf.string)   # list of byte strings\n",
    "tf.io.VarLenFeature(tf.int64)    # list of ints\n",
    "tf.io.VarLenFeature(tf.float32)  # list of floats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A VarLenFeature returns a sparse vector and an additional step is required after decoding the TFRecord:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_data = tf.sparse.to_dense(tf_record['my_var_len_feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to have optional fields in TFRecords. If you specify a default value when reading a field, then the default value is returned instead of an error if the field is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.io.FixedLenFeature([], tf.int64, default_value=0) # this field is optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've covered  \n",
    "ðŸ¤” sharding data files for fast access from GCS  \n",
    "ðŸ˜“ how to write TFRecords. (You forgot the syntax already? That's OK, bookmark this page as a cheat sheet)  \n",
    "ðŸ¤” loading a Dataset from TFRecords using TFRecordDataset  \n",
    "Please take a moment to go through this checklist in your head.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
